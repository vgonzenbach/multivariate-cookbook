[
["index.html", "Advanced Research Methods: Multivariate Analysis Cookbook Chapter 1 Introduction 1.1 Analysis of psychometric questionnaires 1.2 Transdiagnostic or multi-population studies 1.3 Neuroimaging analysis 1.4 Further reading", " Advanced Research Methods: Multivariate Analysis Cookbook Virgilio Gonzenbach 12/15/2019 Chapter 1 Introduction This document is a “recipe card” for how to perform multivariate analysis in R using the ExPosition family of packages, developed by Dr. Hervé Abdi at the University of Texas at Dallas. It showcases the essential code, visualizations and interpretation aids needed for multiple methods of analysis. This recipe card is meant to serve as a quick reference of the general guidelines of use for each of the 8 methods included; it is not meant to be a detailed exposition of the code used to pre-process data and generate the multiple graphs found throughout the document. Those details belong to the Rmarkdown and R source files. As a brief summary, all methods rely on the Singular Value Decomposition (SVD). SVD is a matrix operation that produces 3 lower rank matrices to describe a larger original matrix of data. As such, SVD is a dimensionality reduction technique. Each of the 8 methods differ on the type and number of matrices used for the analysis. Detailed differences between methods are discussed throughout and in relation to particular data. Supplementary computational statistic techniques (e.g., bootstrapping) are also discussed. Below, I detail a broad overview of how these methods may be applied. 1.1 Analysis of psychometric questionnaires Several methods covered in this document can be leveraged to describe the correlations between items of the same psychometric test, or between items from multiple psychometric tests. For example, Principal Component Analysis reveals the intercorrelation of items within a single table. Multiple Correspondence Analysis accomplishes similar objectives when the variables are qualitative. Partial Least Squares Correlation may reveal the common information between two questionnaires at the item-level—a more detailed approach than calculating correlation coefficients between summary scores. Finally, Multiple Factor Analysis allows for the examination of several questionnaires; it is particularly useful for detailed examination of questionnaires that are known to show a major overlap, or that purport to measure the same construct. 1.2 Transdiagnostic or multi-population studies Other methods may be leveraged to compare a set of variables across different subpopulations. Versions of discriminant analysis (e.g., Barycentric Discriminant Analysis and Discriminant Correspondence Analysis) maximize differences between groups, and as such provide the basis for classification algorithms. Dual Multiple Factor Analysis may be used to describe sub-populations for which the same information has been collected. Finally, multiblock variants of barycentric discriminant analysis and discriminant correspondence analysis—i.e. Multiblock Barycentric Discriminant Analysis and Multiblock Discriminant Correspondence Analysis, respectively—combine principles from multi-table techniques and discriminant analysis to examine how different tables (e.g., questionnaires) perform at discriminating between subpopulations. 1.3 Neuroimaging analysis Partial Least Squares Methods enable the examination of multidimensional relationships between brain activity, behavior and experimental designs. Particularly, Partial Least Squares Correlation can be used to reveal the overlap between BOLD signal changes in ROIs and behavioral measurements—e.g., symptoms ratings, self-report questionnaires—or experimental designs—e.g., block or event related fMRI designs. Partial Least Squares Regression follows the same principles when the goal becomes to predict one data table using another, usually to predict behavior from brain activity. Functional connectivity analysis is also facilitated by adapting the methods in this document. For an fMRI experiment, STATIS can be adapted to analyze how activity in multiple ROIs covary across time across either different experimental conditions or different participants. In either case, experimental (or quasi-experimental) groups—e.g., as psychiatric diagnoses—can be contrasted in terms of their functional connectivity by combining STATIS and discriminant analysis. Correlations between functional connectivity and behavior can also be examined by combining STATIS with Partial Least Squares methods. 1.4 Further reading Wiebels, K., Waldie, K. E., Roberts, R. P., &amp; Park, H. R. P. (2016). Identifying grey matter changes in schizotypy using partial least squares correlation. Cortex: A Journal Devoted to the Study of the Nervous System and Behavior, 81, 137–150. Sabaroedin, K., Tiego, J., Parkes, L., Sforazzini, F., Finlay, A., Johnson, B., … Fornito, A. (2019). Functional connectivity of corticostriatal circuitry and psychosis-like experiences in the general community. Biological Psychiatry. Kirlic, N., Aupperle, R. L., Rhudy, J. L., Misaki, M., Kuplicki, R., Sutton, A., &amp; Alvarez, R. P. (2019). Latent variable analysis of negative affect and its contributions to neural responses during shock anticipation. Neuropsychopharmacology, 44(4), 695–702. Klumpp, H., Kinney, K. L., Bhaumik, R., &amp; Fitzgerald, J. M. (2018). Principal component analysis and brain-based predictors of emotion regulation in anxiety and depression. Psychological Medicine. Klumpp, H., Bhaumik, R., Kinney, K. L., &amp; Fitzgerald, J. M. (2018). Principal component analysis and neural predictors of emotion regulation. Behavioural Brain Research, 338, 128–133. Weathersby, F. L., King, J. B., Fox, J. C., Loret, A., &amp; Anderson, J. S. (2019). Functional connectivity of emotional well-being: Overconnectivity between default and attentional networks is associated with attitudes of anger and aggression. Psychiatry Research: Neuroimaging, 291, 52–62. Walther, S., Stegmayer, K., Federspiel, A., Bohlhalter, S., Wiest, R., &amp; Viher, P. V. (2017). Aberrant hyperconnectivity in the motor system at rest is linked to motor abnormalities in schizophrenia spectrum disorders. Schizophrenia Bulletin, 43(5), 982–992. Patel, R., Steele, C. J., Chen, A. G. X., Patel, S., Devenyi, G. A., Germann, J., … Chakravarty, M. M. (2019). Investigating microstructural variation in the human hippocampus using non-negative matrix factorization. NeuroImage. Rajah, M. N., &amp; McIntosh, A. R. (2005). Overlap in the Functional Neural Systems Involved in Semantic and Episodic Memory Retrieval. Journal of Cognitive Neuroscience, 17(3), 470–482. Blake, Y., Terburg, D., Balchin, R., van Honk, J., &amp; Solms, M. (2019). The role of the basolateral amygdala in dreaming. Cortex: A Journal Devoted to the Study of the Nervous System and Behavior, 113, 169–183. Addis, D. R., Pan, L., Vu, M.-A., Laiser, N., &amp; Schacter, D. L. (2009). Constructive episodic simulation of the future and the past: Distinct subsystems of a core brain network mediate imagining and remembering. Neuropsychologia, 47(11), 2222–2238. Bellana, B., Liu, Z., Anderson, J. A. E., Moscovitch, M., &amp; Grady, C. L. (2016). Laterality effects in functional connectivity of the angular gyrus during rest and episodic retrieval. Neuropsychologia, 80, 24–34. Khedher, L., Ramírez, J., Górriz, J. M., Brahim, A., &amp; Segovia, F. (2015). Early diagnosis of Alzheimer’s disease based on partial least squares, principal component analysis and support vector machine using segmented MRI images. Neurocomputing: An International Journal, 151(Part 1), 139–150. McIntosh, A. R. (2012). Tracing the route to path analysis in neuroimaging. NeuroImage, 62(2), 887–890. Yu, M., Wu, Z., Luan, M., Wang, X., Song, Y., &amp; Liu, J. (2018). Neural correlates of semantic and phonological processing revealed by functional connectivity patterns in the language network. Neuropsychologia, 121, 47–57. "],
["principal-component-analysis.html", "Chapter 2 Principal Component Analysis 2.1 The Data 2.2 Correlation Matrix 2.3 Running PCA 2.4 Scree Plot 2.5 (Row) Factor Scores 2.6 Tolerance Intervals 2.7 Confidence Intervals 2.8 (Column) Factor loadings 2.9 Factor Rotations (Varimax) 2.10 Contributions 2.11 Bootstrap Ratios 2.12 Summary", " Chapter 2 Principal Component Analysis Principal Component Analysis (PCA) is a data-reduction technique that extracts the most important information out of a data table of quantitative variables. To accomplish this, PCA computes new variables called principal components through linear combinations of the original variables in a data set. This operation is equivalent to performing the singular value decomposition (SVD) on the original data set, which amounts to performing an eigendecomposition of the covariance or correlation matrix. 2.0.1 Criteria for principal components 2.0.1.1 Maximal Inertia Inertia is a quantity that denotes the total variance in a matrix. The first principal component in a PCA-solution will capture most of the inertia in a covariance/correlation matrix. 2.0.1.2 Orthogonality The second principal component (and third and so on) also maximizes variance explained. However, this is done under the constraint that this variance be orthogonal (i.e. independent) to variance explained by the first component (or, in general, orthogonal to all other previous components). Thus, the overall set of components that results are mutually independent. This implies that PCA will reveal how many dimensions of variability exist within the data. 2.0.1.3 A Little linear algebra detour! A covariance matrix can be calculated by multiplying a matrix (i.e. data set) by its own transpose. Thus, the diagonal of the resulting matrix will contain the sum of squares for each of the original columns (i.e. variables), and the off-diagonal elements correspond to the sum of cross-products between the variables that intersect at that cell. Obviously, the covariance matrix is symmetric. The correlation matrix is obtained from the covariance matrix via an extra step: the normalization of each column by the total sum of squares. This procedure constrains the elements of the diagonal (i.e. the sums of squares) equal to 1. Thus, all cells will have values between 0 and 1 which correspond to the correlation coefficient. 2.1 The Data To illustrate how to interpret results from PCA, the present chapter analyzes a data table containing 144 observations corresponding to participants who took the Big Five Inventory Questionnaire. Thus, the table contains 44 columns consisting of Extraversion (8), Agreeableness (9), Conscientiousness (9), Neuroticism (8), and Openness (10) items. The observations on this data can be classified into one of two groups—High Episodic Memory and Low Episodic Memory—according to the (quasi-) experimental design. ## Ex1 Ex2 Ex3 Ex4 Ag1 Ag2 Ag3 Ag4 Co1 Co2 Co3 Co4 Ne1 Ne2 Ne3 Ne4 Op1 Op2 Op3 ## 1 5 2 4 4 2 4 5 3 5 4 5 2 3 4 5 5 2 5 3 ## 2 2 2 4 4 3 4 4 4 3 2 4 3 2 2 4 3 5 5 5 ## 3 3 2 4 3 1 1 1 5 1 1 2 4 5 5 1 5 5 4 1 ## 4 2 2 2 2 2 5 5 3 3 4 5 2 5 3 2 4 4 5 5 2.2 Correlation Matrix Since PCA decomposes a correlation matrix, a natural starting point is to visualize said matrix and take note of the strength and directions of the correlation patterns. In this correlation matrix, a couple of features can be highlighted. First, there is a strong pattern of correlation between items of the same type. Second, Neuroticism items are negatively correlated to other types of items. These patterns will drive how PCA displays the information in this table. 2.3 Running PCA PCA is executed with the ‘epPCA’ function from the ExPosition package. The function takes multiple parameters: the data must be preprocessed so that only the quantitative variables of interest are included as parameters to the function. scale = TRUE or ‘SS1’ vs scale = FALSE: the choice of whether or not to scale will depend on the data at hand. In this case, the data is comprised of responses to items from a single questionnaire and all items follow the same Likert scale. Since all items have the same unit (i.e. use the same response scale), not scaling would allow for the preservation of important information. For example, some items may capture more variance than others, or may vary under very specific conditions thus revealing the importance of these conditions. However, in cases where units for the columns differ, not scaling would cause certain columns to ‘illegitimately’ dominate the PCA decomposition of the table. Thus, as a rule of thumb, scaling is necessary when units of the columns differ and optional (but preferable) when units are the same. center = TRUE: the data is centered by default. DESIGN: if there was an experimental design that informed data collection then it can be entered here. This will allow for visualization of group membership. However, other categorial variables, can be entered here too, regardless of whether they are integral part of the experimental design (i.e. posthoc). graph: turned off by preference. resPCA = epPCA(bfi.quant, scale = FALSE, center = TRUE, DESIGN = memoryGroups, graph = FALSE ) 2.4 Scree Plot The scree plot below visualizes the amount of inertia explained by each component: the eigenvalue associated with each component (i.e. eigenvector). 2.4.0.1 A note on the Permutation Test A key question whenever performing PCA on any data is ‘how many components should be kept’? A way to answer this is by distinguishing between significant and non-significant components. This distinction is done all the time in the framework of null hypothesis significant testing. Following the overall reasoning of NHST, if our statistic of interest (i.e. eigenvalue of a component, or total inertia explained by a component) has a value which is unlikely to occur by chance, we may conclude that the observed statistic is (statistically) significant. However, the problem when doing PCA is that it is not evident what the distribution of eigenvalues is under the null hypothesis. The permutation procedure allows for the derivation of a distribution (of eigenvalues) when the null hypothesis is true. This is done by rearranging our data set in a way that breaks any relationships between columns or between rows that might exist—see Berry et al., 2011. The scree plot shows that the permutation test highlighted 5 components as being statistically significant. Visualization of factor scores and loadings is show for these 5 components below. 2.5 (Row) Factor Scores PCA summarizes a data table so that the resulting components capture most of the variance between data points. Therefore, PCA can be thought of as drawing an axis across a multidimensional array of data points, under the condition that projections of the data points onto the first axis are maximized; alternatively, it is the distance between the data points and the axis that is minimized, in a least-squares sense. The concept of a row factor score (or factor scores) corresponds to this geometric projection of data points onto a component’s axis: projection onto axis (i.e. factor scores) = distance of data point from the origin * angle (i.e. loading; explained below). The graph shown below corresponds to a representation of observations (i.e. data points) according to the first and second components. Points are colored by group membership. We can appreciate that the x-axis corresponding to component 1 captures more spread in the data (17% of inertia) relative to component 2. This will always be the case as component 1 extracts the most variance. The means of each group are also displayed. It appears that groups migh differ according to the first component. However, without a within-group variability estimate, there is as yet no way through which conclude that this difference is statistically significant. 2.6 Tolerance Intervals A convex hull that contains all data points in a group is a good way to graph the total range of factor scores associated with that group. The graph shows that the overlap between groups is high, since the majority of the data point can be found at the intersection between the convex hulls. 2.7 Confidence Intervals Confidence intervals for each group’s mean can be calculated by computing the bootstrap distribution of group means and excluding value at the tails (e.g., 2.5% in each direction to obtain a 95% confidence interval). This would be the equivalent to a (post hoc) significant testing. 2.7.0.1 A note about Bootstrapping The Bootstrap method serves as a way to quantify the behavior of population parameters or other statistics. As applied here, the general principle behind the bootstrap is to generate multiple alternate samples for each subsample (i.e. memory groups) so as to simulate their distribution if the experiment were to be replicated. Observations from a group are sampled randomly and with replacement, then the mean for the group is calculated for this distribution. The procedure is repeated a large amounts of times (1000 times in this example) to produce a distribution of group means, and from this distribution the confidence interval of the means is computed for the group. Note that the random sampling in a bootstrap procedure draws an entire row from the dataset. Thus, a key feature of the bootstrap is that it preserves the associations between variables, or the effect found in the data, whereas the permutation test does not. The ellipses graphed below represent the 95% confidence intervals (for components 1 and 2). Groups statistically differ on component 1; the projection of the ellipses onto component 2 overlap therefore this component does not differentiate between groups. To interpret this difference between groups, we will want to know what variables constitute component 1. 2.8 (Column) Factor loadings In PCA, the rows (i.e. observations) of a data set are described by their factor scores or projections; the columns, however, are described by their loadings or correlations. The combination of the two (scaled by the eigenvalues) gives a complete description of the original data. (Recall the formula, “projection onto axis (i.e. factor scores) = distance of data point from the origin * angle (i.e. loading)”). Each variable stands in relation to each of the components and this relationship is described by the (cosine of the) angle between a the variable vector and a component. Note: The factor loadings graphed below have been scaled to represent the total amount of variance in components 1 and 2 (and beyond). This can be done by multiplying the factor loadings (i.e. matrix Q of the SVD) by the diagonal matrix of the eigenvalues. The plot below captures the overall correlation pattern of the table. Interpreting the angles between variables, the plot shows that items of the same type tend to be associated with each other—the tip of the vectors of the same color point toward the same general direction. The plot also reflects the correlation matrix insofar as neuroticism loads negatively on component 1 which reflects the negative correlation between neuroticism items and other item types. Component 2, however, is more difficult to interpret. 2.9 Factor Rotations (Varimax) To aid interpretation, factor rotation can be applied to results of a PCA. Varimax, the most popular rotation method, correspond to a rotation eigenvectors within subspace—e.g., the 5 significant components in this example—so that each item loads onto as few components as possible under the constraint that these components remain orthogonal to each other. Our original correlation matrix showed that there is a clear structure in which items of the same type are strongly correlated with each other. In the graph of the factor loadings, component 1 reflects the overall correlation pattern between item types, not within item types. Therefore, performing a varimax rotation would clarify the factor structure so that each component reflects the within-type correlation for a specific item type. 2.9.1 Running varimax The code below is used to run varimax on the results of a PCA. Based on the orignal scree plot I decided to keep 5 dimensions. vari.PCA = epVari(resPCA, dim2keep = 5) This screeplot shows this dimensions after rotation: the eigenvalues have less of a spread, indicating that they are represented more equally. 2.9.2 The effect of varimax The plot of factor loadings after rotation shows the effect of varimax. The difference between group means becomes more evenly distributed across the first two components. Thanks to the simplified structure, it is evident that the first two components reflect Openness and Extraversion. Thus, the High memory group is higher than the Low memory group in both Openness and Extraversion. To showcase the full effect of a varimax rotation, all the factor loadings for the significant components are graphed below. The plots of components 3 and 4 show that they correspond to Neuroticism and Conscientiousness, respectively. The memory groups do not differ in either of these personality dimensions. These plots show components 3 and 5. Component 5 corresponds to Agreeableness and there is no difference between groups here. In summary, varimax revealed the underlying psychometric model in the BFI: that there are 5 largely independent factors of personality measured by this questionnaire. 2.10 Contributions Another tool to interpret the components is to calculate contributions. Contributions reflect how important a column (or row) is to the interpretation of an eigenvector/component. The contributions can be calculated for both the rows and the columns by squaring the factor scores for an element and dividing by the eigenvalue in question. Columns (or rows) with a (squared) contribution above the average should be used to interpret a component. Graphing row contributions would be a good way to detect outliers. In this example, however, I only show the contributions for the components. The barplots for the contributions shown above confirm what we already concluded from graphing the factor loading. Since contributions are derived from factor loadings (by loadings, it is meant the “scaled” loadings that have been multiplied by the corresponding eigenvalue), it naturally follows that this information is replicated. Graphing contribution does make explicit, however, the threshold for importance and highlights important variables only, making the graph more readable in some cases. 2.11 Bootstrap Ratios In general, a bootstrap ratio is a quantity akin to a t-test, calculated from a bootstrap distribution instead of a theoretical t distribution. In the context of PCA, bootstrap ratios are almost always used to test the stability of the contributions (of the variables or observations). The procedure is simple. Bootstrapping—i.e. running PCA multiple (i.e. 1000) times with different bootstrap samples each time—generates a distribution of contribution values from which we can extract the mean and the standard deviation for each variables. The mean contribution is then divided by its corresponding (mean) standard deviation to produce the bootstrap ration. Bootstrap ratios more extreme than 2 (or -2) reflect that that variable’s contribution is statistically significant (p &lt;.05). This threshold of 2 can be modified to account for multiple comparisons. The threshold for this example is equal to 3, corresponding to p &lt; .001. The bootstrap ratios shown below confirm the stability of variable contributions. 2.12 Summary PCA (with a varimax rotation) was used in this example to reveal the underlying structure in the BFI questionnaire. Results showed that the BFI capture variability in personality across 5 different dimensions: Openness, Extraversion, Neuroticism, Conscientiousness and Agreeableness. Further, High and Low Episodic memory groups showed differences in their levels of Openness and Extraversion. In further analyses, I explore the correlation between memory and these personality dimensions further. "],
["correspondence-analysis.html", "Chapter 3 Correspondence Analysis 3.1 Data 3.2 The contingency table transformed: table of deviations. 3.3 Running CA 3.4 Scree Plot 3.5 Asymmetric plot 3.6 Symmetric Plot with CDC 3.7 Symmetric Plot with data from 2004 3.8 Contributions 3.9 Bootstrap ratios 3.10 Summary", " Chapter 3 Correspondence Analysis Correspondence Analysis (CA) is often described as an adaptation of PCA for categorical data. This means that CA extracts the important information from categorical variables in a way that can be interpreted geometrically. A key difference is that the data analyzed in CA is not a covariance/correlation matrix as in PCA. Instead, CA analyzes a contigency table between two categorical variables. Prior to running CA, the counts in a contingency table are transformed to instead reflect probabilities. This is equivalent to dividing the matrix by its sum. Then, the factor scores are calculated through the generalized singular value decomposition (GSVD) of the probability matrix (i.e. Z). A key difference between GSVD and SVD is that GSVD accounts for masses and weights as added contraints. 3.0.1 Masses and Weights Masses correspond to rows and weights correspond to columns. These quantities reflect the importance of a row or column, respectively; they amount to the proportion of cases in a row (or column) relative to the rest of the table. For example, if a column is not used often then it contains more information since rare cases allow us to discriminate better. Thus, the weights (and masses) are calculated as the inverse of the proportion this column (or row) represents in the total table. 3.0.2 Two kinds of factor scores Another difference between PCA and CA is that CA calculates two set of factor scores: one for the rows and one for the columns. Thus, the space between rows and column factor scores is interpretable—unlike PCA, where, for comparing observations to variables only the angles and not the distances are compared. 3.1 Data The data analyzed in this example is a contingency table containing counts of how many deaths of each type are reported in different news media. Type of death can be found in the rows, while news outlet is found in the columns. Let’s introduce the concept of active and supplementary data. (This concept is relevant to all multivariate analyses in this document but is introduced here due to its central role in this particular example of CA.) ## CDC Google Guardian NYT ## Heart Disease 629474 437 508 577 ## Cancer 613832 842 3049 3128 ## Lower Respiratory Disease 154596 534 383 271 ## Car Accidents 159340 436 677 442 ## Stroke 101328 1042 1246 1171 ## Alzheimer&#39;s Disease 116103 351 1 234 ## Diabetes 80058 771 541 555 ## Pneumonia &amp; Influenza 51537 721 555 671 ## Kidney 56021 945 34 41 ## Suicide 44876 389 3370 2497 ## Homicide 19103 652 5597 5285 ## Terrorism 0 300 7987 8266 ## Overdose 58335 572 68 92 3.1.0.1 Active data The term “active data” refers to the data that is used to compute factor scores, or the data inputted in the CA function. In carrying out any multivariate analysis, it is important to choose which data will be active and which data will be left out. In the data shown above, it is evident that deaths reported by the CDC far outnumber those of other news outlets. Consequently, if this column were to be included the contribution of CDC would dominate the CA results, leaving little room for other news outlets to contribute. Thus, this column is not included as active but instead as a supplementary column. 3.1.0.2 Supplementary data Any data that is not used to calculate factor scores can be projected onto the vector space of the CA results to aid interpretation. This is the meaning of supplementary data. Along with including the CDC as a supplementary column, the rows for Suicide, Homicide and Terrorism will be excluded from the active data set and included as supplementary rows. 3.2 The contingency table transformed: table of deviations. The table displayed below is not a correlation matrix like in PCA. Instead, it is the result of applying a transformation onto the contingency table so that each cell represents the deviation from expectation (based on the row and column totals). Thus, the big red dot at the intersection of ‘Google’ and ‘Cancer’ signifies that the amount of cancer deaths reported by Google fall far below the mean of cancer deaths reported by all news sources in the data. 3.3 Running CA CA is executed by the ‘epCA’ function in the ExPosition package. The code below shows what parameters are relevant for running CA: how the data is arranged/transposed (i.e. which categories are in the rows and which in the columns). This is especially important in asymmetric CA solutions. whether column factor scores will be symmetric or asymmetric. resCA.sym &lt;- epCA(X, symmetric = TRUE, graphs = FALSE) 3.4 Scree Plot The scree plot below show that the data can be described by two dimensions. And the permutation test shows that both components are statistically significant. However, total variance explained varies widely between the two components: the first explains 95% of the variance and the second only 5%. This should be kept in mind when interpreting factor scores. 3.5 Asymmetric plot As mentioned above, a defining characteristic of CA is that the relationship between factor scores for the rows and the columns can be represented in two ways: symmetrically or asymmetrically. The difference between the two is on whether column factor scores are weighted according to (the inverse of) the proportion of inertia relative to the whole table. The plot below is called an asymmetric biplot. In this kind of graph, the space between columns is represented with the weights of each column in mind (i.e. weights multiply the distance of each column factor score from the origin). The purpose of graphing an asymmetric biplot is that it allows for direct interpretation of the space between a row and a column. The relative distance between a row and all columns provides information over how characteristic that row is for each column. Since CA analyzes the probability matrix of deviations from the average, factor scores nearest to the origin are not particularly characteristic of any particular column but instead characterize the average column. Conversely, factor scores at the extremes provide more information about particular cases. For example, reporting deaths related to Kidney failure is closer to Google than to other news sources, therefore reporting this type of death characterizes Google relative to other news outlets. Deaths by Alzheimer’s Disease are closer to Google than to other news outlets as well, but between NYT and the Guardian AD deaths gravitate towards NYT. 3.5.0.1 Interpreting components The first component explains most of the variance in this table. Since it separates between Google and NYT and The Guardian, we can conclude that this component represents traditional vs. crowd-sourced media. Row factor scores supply information about what kinds of deaths are reported across each type of media. The second component explains less variance. It might only be distinguishing between NYT and The Guardian, a not so interesting point; however, we might be able to (perhaps) generalize this pattern to British vs US media, but having more columns representing these media would be helpful to make this interpretation. 3.5.0.2 Supplementary Projections: Homicide, Suicide, Terrorism. Deaths related to Homicide, Suicide and Terrorism where excluded from the main, active data set. Yet by projecting these as supplementary rows onto the vector space, information can be acquired from these data without them dominating the results of the analysis. The projected factor scores show that these types of death are extremely characteristic of the traditional new outlets and they are reported equally in NYT and The Guardian. This helps further characterize what distinguishes between traditional and crowd-sourced news outlets: more sensational but rarer deaths tend to be reported more in the newspaper. 3.6 Symmetric Plot with CDC Symmetric Plots allow for the examination of one type of factor score (i.e. either rows or columns) relatively well since they accurately represent column factor scores from the point of view of total variance explained. Further, we can easily project supplementary elements (again, either rows or columns depending on the active factor scores being represented) to gain additional information on the resulting components. This graph shows all columns from the original data (with CDC as a supplementary projection). Symmetric plots show the column scores in relation to the amount of variance explained. Thus, the plot shows the distance between factor scores on the left and on the right is much wider than the distance going up or down. By focusing on CDC factor scores, we can strengthen our interpretation of components 1 and 2. In relation to component 1, the CDC falls relatively close to the middle. Since, the CDC is the most objective source for all types of deaths (in the US), we can interpret deviation of columns relative to the middle as deviations from objectivity: traditional news sources report sensational and rare deaths, however, this does not mean that Google reports “the true proportions” of deaths. Rather it seems that both extremes deviate from objectivity: and what component 1 might be reflecting is how much people seek out information on certain deaths (e.g., people might spontaneously worry about overdose more frequently than being a victim in a terrorist attack). On component 2, the CDC loads highly on the positive end (4 times closer to NYT than The Guardian). Since the CDC is based in the U.S., this lends support to the interpretation that dimension 2 captures a regional effect, such that death reports on NYT more closely align with total deaths reported in the US. However, when compared to the effects captured by the first component, this regional effect seems to be rather small. What matters most is the kind of news outlet (i.e. traditional vs. crowd-sourced). 3.7 Symmetric Plot with data from 2004 We have another interesting research question: we want to know how the reporting of certain deaths has changed over time. To answer this question, we supplementary project a table with the same columns and rows but for 2004 deaths and display the results in a symmetric map. Examining the graph above shows that the most striking change has been on how Alzheimer’s Disease deaths are reported. The factor scores for AD have moved to the left and up. This means that, relative to 2004, Alzheimer’s Disease deaths have started to be reported more by traditional news media. Similar patterns can be appreciated for Overdose, Diabetes, Heart Disease and Cancer. Further, the vertical change indicates that AD started to be more reported by NYT in recent years. (Perhaps this could be generalized to all US media if we had more data). 3.8 Contributions Contributions in CA are interpreted just as in PCA. The main difference is that, in CA, contributions to both row and column factor scores are of interest. (They can also be of interest in PCA if there are clusters of participants). The contribution barplots confirms our interpretation: component 1 distinguishes between Google and traditional news outlets, while component 2 distinguishes between NYT and The Guardian (or US vs British Media if the pattern were to be generalized). From the point of view of the rows, the contribution barplots isolate rows that are of special importance for understanding difference across news outlets. Contributions of the rows show that Google reports more Kidney and Overdose death than traditional outlets. Conversely, Reporting cancer deaths is relatively uncharacteristic of Google when compared to other news outlets. 3.9 Bootstrap ratios Bootstrap ratios are computed to test the stability of contributions. In general, bootstrap ratios show that the contributions are stable. In particular, more information can be gleaned from contributions to component 2, further clarifying differences between the Guardian and NYT (or British and US media in general). 3.10 Summary CA was used to analyze to a contingency table of type of death reported (rows) by news outlet (columns). The analysis revealed that news outlets can be classified according to a traditional vs crowd-sourced dimension (component 1), and that this dimension captures distinctions between everyday concerns and rare diseases (or human-caused death), the latter being more characteristic of traditional news. Further, a second dimension corresponding to a possible regional effect was revealed by the inclusion of CDC as a supplementary column. Thus, NYT and the Guardian differed on this dimension (with NYT reporting deaths that more closely align with total deaths in the US). "],
["multiple-correspondence-analysis.html", "Chapter 4 Multiple Correspondence Analysis 4.1 The Data 4.2 Correlation Matrix for Recoded Data 4.3 Running MCA 4.4 Scree Plot 4.5 (Row) Factor Scores 4.6 Column Factor Scores 4.7 Rotation 4.8 Contributions 4.9 Bootstrap Ratios 4.10 Summary", " Chapter 4 Multiple Correspondence Analysis Multiple Correspondence Analysis (MCA) is an extension of correspondence analysis to deal with more than 2 categorical variables. MCA can also be used to analyze quantitative variables after a few pre-processing steps. In the case of quantitative variables, MCA can reveal non-linear effects that PCA would not be able to reveal. Thus, it is a good idea to try to “replicate” any PCA results with MCA to detect non-linear effects that might be of relevance. 4.0.0.1 Disjunctively-coded matrix MCA analyzes a disjunctively-coded matrix in which columns represent a particular level of a particular category while rows represent observations (e.g., participants). The disjunctively-coded matrix contains only 0’s and 1’s: a 1 marks the level of each categorical variable to which an observation belongs. For example, for a male participant the portion of the table describing gender would look like “1 0”, marking “true” for male and “false” for female. 4.1 The Data This example of MCA uses the same BFI data. However, the data has been transformed by binning. Essentially, groups were created for each of the variables so that each observation belongs to a single group. The histograms below show how the data was cut. Most of the variables were recoded so that they would be described by 3 groups. In general, it is ideal to try to maintain equal sample sizes between the groups of a given variable. When this was not possible, histograms where cut into 2 (e.g., Neuroticism item 3). 4.2 Correlation Matrix for Recoded Data Displayed is the correlational pattern between the items from the perspective of the new coded data. The overall pattern does not change drastically from that shown in the PCA chapter. One noticeable difference is that Agreeableness does show a clearer structure. 4.3 Running MCA MCA is executed with the ‘epMCA’ function from the ExPosition package. The main parameter is ‘make_data_nominal’, which is set to FALSE here since the data has already been pre-processed. 4.4 Scree Plot The screeplot shows that there are 7 dimensions whose eigenvalue achieved significance. Therefore, we want to pay special attention to dimensions 6 and 7, since this is a clear difference with the PCA results. 4.5 (Row) Factor Scores Factor Scores in MCA are calculated in the same way than in CA, and like in PCA, correspond to the projection of a row (or column) onto a component. The plot for dimensions 1 and 2 shows that there is high overlap between the memory groups. Dimension 1 captures most of the variance 47%: this is a lot more than the variance captured by PCA on the first dimension (17%). The confidence intervals show that the groups significantly differ on dimension 1. Graphing the factor scores for the columns will be more informative to interpret this component 4.6 Column Factor Scores In the following graphs, gray is used to color variables whose contributions are not important. Thus, only colored variables should be used to interpret a component. The graph shows that component 1 captures a mix of Extraversion, Openness, Conscientiousness and Agreeableness items, while component 2 reflects mostly Neuroticism. 4.7 Rotation To compare with results from PCA, a varimax rotation is applied to the 7 significant components. What follows are the graphs of (row and column) factor scores for the rotated dimensions. 4.7.1 Dimension 1 and 2 The rotated factor scores make intepretation clearer. Component 1 captures Openness while Component 2 captures Neuroticism. 4.7.2 Dimension 3 and 4 Dimensions 3 and 4 correspond to Agreeableness and Extraversion, respectively. Groups appear to differ on Dimension 4 (Extraversion), but effect is less clear than in PCA. 4.7.3 Dimension 5 and 6 Dimension 5 corresponds to Conscientiousness. However, it is unclear what dimension 6 corresponds from looking at this graph. It appears it might be a mixture of moderate scores in Neuroticism, Extraversion, and Openness. 4.7.4 Dimension 6 and 7 Dimension 7 seems to also capture moderate levels of personality, particularly, agreeableness and conscientiousness. Although this effect illustrates the differences between PCA and MCA nicely, the groups overlap in this dimension. Looking at this data in isolation, it is unclear whether these non-linear effects are merely an artifact of how the variables were binned. 4.8 Contributions Contributions for dimension 1 to 5 confirm the pattern from the factor loading plots. Contributions to dimension 6 seem mostly driven by Extraversion, while contributions to dimension 7 are mixed. Since Extraversion was more consistently binned into 3 groups, component 6 could represent an artifact steeming from our binning strategy. However, since all variables were binned using this criteria, MCA reveals that not all the subscales of the BFI are equally sensitive: the Extraversion subscale discriminates between different levels better. 4.9 Bootstrap Ratios Bootstrap ratios confirm that all dimensions are stable, even dimensions 6 and 7. Therefore, MCA suggests non-linear effects of personality dimensions, and that the BFI might be more tailored to caturing non-linear effects associated with Extroversion. 4.10 Summary MCA is an extension of CA used to analyze a matrix of observations by (nominal) variables. In this analysis, MCA replicated and extended the results from PCA. MCA revealed a non-linear effect of Extraversion. These non-linear effects of the BFI might be useful information when comparing performance with other personality scales or when further developing the BFI itself. If moderate levels of Extraversion (and other personality dimensions) differentially predict certain outcomes, then it would be beneficial to utilize a psychometric scale that can capture such effects. However, the existence of non-linear effects did not particularly illuminate the data from the perspective of our main research question: ‘how are personality factors related to memory’? Therefore, if overlapping results like these were to be published, PCA would likely suffice, and perhaps be prefereable due to its wider accessibility. "],
["barycentric-discriminant-analysis.html", "Chapter 5 Barycentric Discriminant Analysis 5.1 The Data 5.2 Group by Variables Matrix 5.3 Running BADA 5.4 Scree Plot 5.5 Factor Scores 5.6 Contributions 5.7 Bootstrap Ratios 5.8 Classification: Confusion matrices 5.9 Summary", " Chapter 5 Barycentric Discriminant Analysis Barycentric Discriminant Analysis (BADA) is a version of discriminant analysis that utilizes the eigen-decomposition to assign observations to a-priori groups. As such, BADA is a classification algorithm whose main purpose is to determine how well certain data can classify groups of observations. To do this, BADA utilizes the eigen-decomposition to maximize between-groups variance. This is accomplished by an SVD of the groups-by-variables matrix (instead of observations-by-variables as in PCA), which computes linear combinations of variables which will maximally distinguish between groups. Each row of the groups-by-variables matrix represents the barycenter of a group, hence Barycentric Discriminant Analysis. After computing the vector space, the invidual observations composing the groups are projected and classified according to which group barycenter is the least distant. 5.1 The Data In this example, the same BFI data is utilized to illustrate BADA. Results of PCA revealed that memory groups differ on their amount of extraversion and openness. In the results of PCA, this pattern was implicit, since the memory group design informed data collection and groups were equal in sample size. As such, an (unrotated) PCA indirectly captured a dimension which corresponded to variance due to the experimental factors. In the BADA algorithm, the goal of capturing between-group variance becomes explicit. Also, there is the added step of categorization, which provides an additional criterion for evaluating the correlation of memory and personality. Nevertheless, results from BADA should closely resemble those from PCA for this particular data. 5.2 Group by Variables Matrix Below, the matrix to be analyzed by BADA is visualized. For each variable, the element in a row show the deviations of the group means from the grand mean. Since there are only two rows (i.e. groups), each value has its mirror value in the other row: a positive value in one row would be equal but negative on the other row. A strong pattern of differentiation between the memory groups is revealed on scores of Openness and Extraversion. 5.3 Running BADA BADA is executed by the ‘tepBADA’ function in the TExPosition package. In this particular implemention, the columns were not scaled by their standard deviation. As in PCA, this is because all columns have the same unit: the same Likert scale. Thus, not scaling preserves variance. resBADA &lt;- tepBADA(bfi.quant, DESIGN = memoryGroups, graphs = FALSE, scale = FALSE) ## Warning in pickSVD(datain, is.mds = is.mds, decomp.approach = decomp.approach, : ## Solution has only 1 singular vector (or value). Zeros are appended for plotting ## purposes. 5.4 Scree Plot The dimensionality of the resulting vector space, as in all SVD-based methods, is constrained by the number of rows and columns. In this case, the number of dimensions cannot exceed the number of groups. Thus, BADA returns one single dimension which account for all the inertia in the table. 5.5 Factor Scores Factor Scores for the rows can be calculated (and graphed) just as in PCA. However, due to there only being one dimension in this particular implementation of BADA, factor scores are represented with histograms. The plot shows that the distribution for the two groups is distinguishable yet there is a high amount of overlap. A good way to quantify this overlap is to rely on the accuracy of the classification procedure (see Classification section). 5.6 Contributions Since the present eigen-decomposition consist of only one dimension, contributions are plotted in the place of factor loading. A key attribute of the contributions is that they are a scaled version of the loadings (or factor scores): contributions for columns (or rows) are calculated by squaring each loading and then dividing by the eigenvalue of that dimension. As such, contributions provide similar information than factor scores, with the added bonus of a threshold (i.e. the mean contribution for the given dimension) to determine relative importance. As predicted, the contribution barplot reveals a pattern highly resembling the PCA results. Extraversion and Openness discriminate between memory groups. BADA, however, suggests that conscientiousness might also play a role between group differences. 5.7 Bootstrap Ratios As a refresher, bootstrap ratios quantify the stability of the contributions. Below are the bootstrap ratios for the variable contributions to dimension 1. The overall pattern appears to be stable. The contributions from Openness were the most stable, followed by Extraversion and Conscientiousness. 5.8 Classification: Confusion matrices The key feature that distinguishes BADA from PCA is the classification procedure. Below, I display two confusion matrices, which summarize the performance of this classification procedure. A confusion matrix displays signal detection: the numbers in the diagonal represent instance of correct classification (i.e. hits), while off-diagonal elements represent incorrect classification (i.e. false alarms or misses depending on point of view). 5.8.0.1 Fixed-Effect Confusion Matrix ## .High .Low ## .High 53 26 ## .Low 19 46 The Fixed-Effect Matrix reveals how the classification procedure performs for each group. The High group was classified more accurately than the Low group in this instance. Overall accuracy is also reported below and should be intepreted as the improvement over randomness. This index is calculated by adding the elements in the diagonal and scaling by the total cases that were classified: it represent the percentage of correctly classified cases. ## [1] 0.6875 The algorithm performs moderately well. However, as the section below shows, accuracy estimates calculated form the fixed effect matrix overestimate fit. 5.8.0.2 Random-Effect Cross-Validation A goal of developing classification algorithms is to have a procedure where observations can be classified by using only the data at hand and with no a priori information of group membership. Therefore, what is of real interest—from an algorithm persepctive, instead of a descriptive perspective—is how BADA performs with new observations that have not been factored in the calculation of the eigen-vector space. The implementation of the random-effect classification reported below is known as a ‘leave-one-out’ cross-validation, or ‘LOO’. The LOO consists of running BADA as many times as there observations but leaving each observation out of the SVD one time. The left-out observation is classified each time and the result is saved onto a matrix, which contains group assignments for each of the observations. The results for this example are displayed in a confusion matrix below. ## .High.actual .Low.actual ## .High.predicted 50 33 ## .Low.predicted 22 39 Examining the matrix, it is apparent that the leave-one-out procedure performs worse than the fixed-effect classification. This is always the case since the fixed confusion matrix is biased by virtue of each observation being included in the SVD used to classify it. Theerefore, the accuracy of the loo provides a less biased estimate than the one show above. ## [1] 0.6180556 Thus, BADA improves from chance classification by only 10% with ‘out of sample’ observations. Ultimately, a whole new set of observations will provide the best estimate for the performance of the classificaiton algorithm, however, the loo is next best method available. 5.9 Summary BADA performs a generalized PCA on the groups-by-variables matrix, maximizing between-group variance. Observations are project onto the resulting vector space and classified according to the closest barycenter. The performance of BADA is evaluated by the accuracy the loo cross-validation methods. In this instance, BADA revealed that Openness and Extraversion distinguish between memory groups, yet the random-effect accuracy index revealed that this effect is not that much better from chance. "],
["discriminant-correspondence-analysis.html", "Chapter 6 Discriminant Correspondence Analysis 6.1 The Data 6.2 Running DiCA 6.3 Scree Plot 6.4 Factor Scores 6.5 Contributions 6.6 Bootstrap Ratios 6.7 Fixed-Effect Classification 6.8 Random-Effect Classification 6.9 Summary", " Chapter 6 Discriminant Correspondence Analysis Discriminant Correspondence Analysis (DiCA) is a version of discriminant analysis that utilizes categorical variables to arrive at a optimal discrimination between groups. Like BADA, DiCA analyzes the groups-by-variables matrix. However, like CA, DiCA carries out the generalized singular value decomposition wiht masses and weights. Because of the high overlap between these methods, this report focuses on key differences between BADA and DiCA results rather than a independent exposition of DiCA. 6.1 The Data This report includes DiCA results for the analysis of the Recoded BFI matrix used in MCA (in which each variable was binned so that group sizes were as equivalent as possible). ## Ex1 Ex2 Ex3 Ex4 Ex5 Ex6 ## 1 3 1 2 2 3 1 ## 2 1 1 2 2 3 1 ## 3 1 1 2 1 1 1 ## 4 1 1 1 1 1 1 6.2 Running DiCA DiCA is executed by the ‘tepDICA’ function in the TExPosition package. A unique feature of DiCA is the parameter group.masses which computes the relative importance of each group based on how many observations each contains. In a balanced design, this parameter can be ignored—as it is done here. resDiCA &lt;- tepDICA(bfi.quant.Recoded.dis, make_data_nominal = FALSE, #group.masses = g.masses, # weight = rep(1, nrow(XYmat)), DESIGN = memoryGroups, graphs = FALSE) ## Warning in pickSVD(datain, is.mds = is.mds, decomp.approach = decomp.approach, : ## Solution has only 1 singular vector (or value). Zeros are appended for plotting ## purposes. 6.3 Scree Plot As in BADA, the dimensionality of the DiCA results will usually depend on the number of groups (assuming the number of groups is less than the number of varianbles). Here, only one dimension is obtained since there are only two groups. 6.4 Factor Scores The factor scores are displayed in histograms since there is only dimensions. Their distribution is slightly less smooth than in BADA but the non-overlap between groups is comparable. 6.5 Contributions Signed contributions are reported for each of the BFI variables. As in BADA, Extraversion and Openness drive group differences. However, small contributions from other personality items—Agreeableness, Neuroticism—reach significance. 6.6 Bootstrap Ratios Bootstrap ratios reveal that contributions from items other than Openness and Extraversion are not stable. The low-stability of these contributions suggests that BADA is more stable than DiCA in this data, likely due to the pre-processing step of binning variables and the many ways it could have been different. 6.7 Fixed-Effect Classification ## .High .Low ## .High 51 22 ## .Low 21 50 The confusion matrix above suggests that DiCA may be more symmetrical at classifying observations than BADA. The hit rate for the groups is equivalent while BADA tended to overclassify observations as belonging to the High memory (i.e. false alarms). ## [1] 0.7013889 Overall accuracy for DiCA remains comparable to BADA (if only slightly better). 6.8 Random-Effect Classification The random-effect confusion matrix suggests that the symmetry of classification of DiCA remains stable when classifying ‘out-of-sample’ observations. However, as evidenced by the accuracy index, overall performance is slighly worse than BADA. ## .High.actual .Low.actual ## .High.predicted 44 30 ## .Low.predicted 28 42 ## [1] 0.5972222 6.9 Summary DiCA performs a discriminant analysis when the data describing each group is categorical. The current analysis showed that DiCA classification was slightly worse than BADA’s, yet a feature that should be remarked is that DiCA was less-biased toward classifying observations as belonging to the high group. It is conceivable than in some instances this feature would be preferable than a slighly better performance. Thus, DiCA remains a viable option for real-world implementations. "],
["partial-least-squares-correlation.html", "Chapter 7 Partial Least Squares Correlation 7.1 The Data 7.2 Correlation Matrix 7.3 Running PLSC 7.4 Scree Plot 7.5 Visualizing Latent Variables 7.6 Visualizing Saliences 7.7 Contributions 7.8 Bootstrap ratios 7.9 Summary", " Chapter 7 Partial Least Squares Correlation Partial Least Squares Correlation (PLSC) is a technique to extract the common information between tables. The SVD in PLSC decomposes, not the original data tables, but the matrix R which is the product of the Z-transformed original data tables. Thus, the each dimension of a PLSC analysis will maximize the covariance between two tables while following the orthogonality contraint of PCA-based methods. Some key differences between PCA are PLSC are highlighted. 7.0.1 Saliences PLSC decomposes the R correlation matrix between tables as three separate but related matrices: U∆VT; where U and V correspond to the matrices of Y and X saliences, respectively, and ∆ correspond to the diagonal matrix containing the singular values (i.e. the square root of the eigenvalue). Saliences are similar to loadings and as such, they should be interpreted as the correlation between a variable and a component (i.e. the coefficient of a given variable in the linear combination that results in an eigenvector). The key difference here is that PLSC computes two sets of saliences whereas PCA computes only one set of loadings. This is because latent variables and saliences are compute for both table X and Y, and each orrespond to one and only one table (e.g., saliences for latent variable x are only computed for variables from table X, not Y). 7.0.2 Latent Variables Latent variables are similar to factor scores in PCA. However, they are computed differently. While in PCA factor scores are computed by multiplying the matrix P by ∆, in PLSC multiplying U by ∆ does not give the factor scores (of the rows). Instead, latent variables are computed by projecting the Z-normalized data onto the matrix containing its saliences. Therefore, each singular value (or eigen-value) is associated with two latent variables (instead of one “component” as in PCA). [Note: although multiplying U by ∆ does not give the row factor scores, this operation is still useful in that it is the equivalent of scaling loadings to their associated total variance—an approach that was used in the PCA example.] A characteristic of PLSC is that the two sets of latent variables associated with the same singular value are correlated (that is after all the purpose of executing PLSC); however, two latent variables associated with different singular values are orthogonal. Thus, PLSC has its own orthogonality constraint. 7.1 The Data In this example, I re-utilize the BFI data as the first table. The second table is the Survey of Autobiographical Memory (SAM). SAM consists of 24 items each measuring a certain component of autobiographical memory: Episodic Memory (8 items), Semantic Memory (6), Spatial Memory (6), and Future Prospection (6). Like the BFI, the SAM utilizes a 5-point Likert scale. ## E1 E2 S1 S2 P1 P2 F1 F2 ## 1 5 5 5 5 4 4 4 3 ## 2 4 4 4 4 4 5 4 4 ## 3 1 5 2 5 4 2 1 1 ## 4 5 4 4 5 1 2 5 5 Performing PLSC on the BFI and the SAM attempts to answer the following research question: how are personality factors related to components of autobiographical memory? Note that this is the same research question of PCA, but from another point of view. 7.2 Correlation Matrix PLSC decomposes the correlation matrix between two tables. Therefore, visualizing this correlation matrix will allow us to anticipate the results of the analysis. By looking at the correlation matrix, we can distinguish what seem to be “hotspots” of strong correlations, namely the corners of the graph. This suggests that Extraversion and Openness will be highlighted, along with Episodic Memory and Future Prospection, in the PLSC results. 7.3 Running PLSC PLSC is executed by the ‘tepPLS’ function in the TExPosition. Important parameters are: the two tables that will be used in the analysis; and whether to normalize within each table. This example is not normalized for either scale, since both use Likert scales. 7.4 Scree Plot The scree plot below shows one dimension that explains 82% of the variance. The rest of dimensions are below the Kaiser criterion (i.e. eigenvalue = 1) and likely represent noise. Note that a few dimensions past the “elbow” are highlighted as significant by the permutation test. This is because the null hypothesis in PLSC is that all correlation coefficients in the R matrix are zero. Since there are so many, at least a few of these will surpass the significance threshold by chance alone. Therefore, the inference battery in PLSC is overpowered in most cases. 7.5 Visualizing Latent Variables In PLSC, we are interested in the association between two tables. To understand this, we visualize the Latent variables of X and Y corresponding to the same singular value. The graph shows that dimension 1 distinguishes between groups, especially Latent variable y. [Note: “Dimension 1” corresponds to both latent variables together.] Thus, the SAM serves to validate the grouping criteria used as the “experimental” design. 7.6 Visualizing Saliences The saliences for dimension 1 are graphed to understand how each variable correlates to the laten variable. The barplots show the correlation of each element of the table to the corresponding Latent Variable. For the BFI, Openness items correlated particularly high with Latent variable X, followed by Extraversion and then Conscientiousness. For SAM, Episodic Memory and Future Prospection correlate highly with Latent variable Y. From this graph, we can already see that the main relationship between the BFI and the SAM is that of Openness and Extraversion to Episodic Memory and Future Prospection. Conscientiousness and the other aspects of memory might also participate. Therefore, the next aim is to define a treshold to interpret these relationships. 7.7 Contributions Contributions provide a treshold for the role variables play. This is true for all PCA-based methods. Here the contribution barplots strongly suggest that, for the BFI, Openness and Extraversion are the main contributors to Latent variable X, while for the SAM, Episodic Memory and Future Prospection are the main players. 7.8 Bootstrap ratios As usual, bootstrap ratios ratios allow us to test the stability of each of the contributions: significant bootstrap ratios show that a given item contributes in simulated replications of the study (i.e. bootstrap samples). The bootstrap ratios for Dimension 1 show that the contributions from the items of interest are stable. 7.8.0.1 A note on Dimension 2 The screeplot indicated that only one dimension was significant. Thus, latent variables and saliences for dimension 2 and on are not graphed in this report. For illustration purposes, however, the contributions and bootstrap ratios for dimension 2 are found below. Contribution barplots suggests that dimension 2 might be characterized by Agreeableness and several items on the SAM. However, the bootstrap ratios show that these contributions are not stable, and therefore dimension 2 should not be interpreted. 7.9 Summary PLSC maximizes the covariance between two tables. It corresponds to an SVD of the correlation matrix between the tables. In this analysis, PLSC was used to show the correlation between personality dimensions and autobiographical memory. Results showed that Openness and Extraversion are related to Episodic Memory and Future Prospection. In addition, latent variable Y validated the grouping criteria. "],
["multiple-factor-analysis.html", "Chapter 8 Multiple Factor Analysis 8.1 Data 8.2 Correlation Matrix 8.3 Scree Plot 8.4 Factor Scores 8.5 Factor loadings 8.6 Partial Factor Scores 8.7 Contributions 8.8 Summary", " Chapter 8 Multiple Factor Analysis Multiple Factor Analysis is a multi-table adaptation of PCA. Similar to PCA, MFA summarizes a matrix by performing its singular value decomposition. However, to prevent a single table from dominating the overall decomposition—which happens when one of the tables has a higher inertia than the others—MFA equalizes the contributions from the multiple tables. This equalization amount to performing a “preliminary” PCA separately on each table and then dividing them by their own first singular value. Dividing by the first singular value (i.e. the square root of the eigen-value; a quantity akin to standard deviation) amounts to scaling each table so that it can be compared against the others—analagous to a Z-score standardization but for tables instead of individual variables. An equivalent procedure is to carry out a generalized singular value decomposition of the grand matrix by weighting each column by a quantity known as alpha—the inverse of the first squared singular value for a given table— all columns of a given table share the same weight. In this example, I use MFA to analyze three tables that describe the same observations but different variables. Nevertheless, MFA can be used to analyze different measurements at different timepoints as long as the observations remain the same; dual-MFA is a variation where the same variables can be analyzed on different populations. 8.1 Data The three tables used in this analysis are the BFI, SAM and OSIQ. The OSIQ, short for Object-Spatial Imagery Questionnaire, is a self-report questionnaire that captures individual preferences in visualizing. Object imagery is the tendency to visualize through colors, shapes, size and other specific visual qualities of objects; Spatial imagery is the tendency to “visualize” abstract representations of relationship and relative positions between objects and concepts. ## ID s01 s02 s03 o04 s05 s06 o07 ## 1 164 4 3 3 4 2 1 4 ## 2 220 2 4 2 4 2 4 4 ## 3 222 5 5 1 2 5 1 1 ## 4 444 4 4 2 2 5 5 5 Object imagery items are marked with an “o”, while spatial imagery items are marked with “s”. 8.2 Correlation Matrix After the scaling procedure described above—where each table is divided by its first singular value—MFA amounts to a PCA of the concatenated tables. [Recall that this, in turn, amounts to the SVD of the data table, or the eigen decomposition of the correlation/covariance matrix.] Below, the correlation matrix between tables is displayed, with lines separating the tables for ease of interpretation. In the third column, the correlation matrix reveals that spatial and object imagery have dissociable correlation patterns: correlations with object imagery are strongest with Extraversion, Openness and Episodic Memory and Future Prospection. In particular, the strongest inter-table correlations are between the OSIQ and the SAM, therefore we can predict that variables in these table will largely contribute to the first component. ##Running MFA MFA is executed with the ‘mpMFA’ function from the MExPosition package. The function takes the concatenated raw data tables as one matrix or data frame, and it also requires a vector that indexes which columns belong to which table. 8.3 Scree Plot The scree plot below reveals one component that captures a lot of variance (20%) relative to the other components. In total, there may be somewhere between 6 and 7 significant components, as indicated by the elbow test. This document only displays graphical representations for Dimensions 1 and 2 (refer to the source file for graphs of remaining dimensions and their rotated equivalents). 8.4 Factor Scores Just like in PCA, factor scores in MFA represent the projection of observations (data point) onto a component (axis). Unlike PCA, however, MFA produces two sets of factor scores: compromise factor scores and partial factor scores. Compromise factor scores are derived from a weighted averaging of partial factor scores that takes into account the alpha of each table. The partial factor scores, on the other hand, correspond to a projection of each table onto its corresponding table of loadings (Q). The tolerance intervals in the above graph show that groups moderately overlap. The mean of each group displayed here corresponds to the average compromise factor scores for that group. This graph shows that means of high and low episodic memory groups significantly differ on component 1 and to a lesser extent on component 2. This is signaled by the non-overlapping of bootstrapped confidence intervals (ellipses). To understand what drives these group differences, we may interpret the diagonal line separating group means as resulting from variation along a set of (linear combinations of) variables. Graphing factor loadings will reveal which variables correlate highly with this diagonal. 8.5 Factor loadings As in PCA, factor loadings in MFA can be calculated by multiplying the matrix Q (the angles between variables and components) by the diagonal matrix of singular values (i.e. scaling factors corresponding to the variance on each dimension). Graphing the resulting matrix provides us with a visual representation of: (1) the correlation between variables, interpreted as the angle between variable vectors (which always cross the origin); (2) the correlation between variables and components, interpreted as the angle between a variable vector and an axis; and (3) the contribution of a variable to a component, interpreted as the projection of the variable vector onto an axis; (4) the relative inertia explained by each component and its associated variables, interpreted as the length of each axis and the distance between variable vectors and the origin. In this graph, we can appreciate that the first component is dominated by SAM and OSIQ variables, while the second component reflects BFI items. However, a richer interpretation can be arrived at by considering the two components together. We can see that the end of a lot of variable vectors (i.e. the points) agglomerate somewhere around (x = -0.02, y = -0.005). Thus the diagonal crossing this point (approximately) and the origin, is an important feature of this component-solution. Incidentally, this diagonal corresponds with the diagonal between group means, to an extent. Thus, we may interpret differences between group means in part due to Episodic Memory, Future Prospection and Object Imagery. However, other variables are likely at play since the diagonal line projected onto the group means has a slightly different angle; projected outward that diagonal would cross the point at x = -0.02, y = -0.0025. This suggests a contribution from some of the BFI items such as Openness and perhaps Extraversion. We may confirm this interpretation by graphing variable contributions. 8.6 Partial Factor Scores A unique feature of MFA (and other multi-table techniques) is the inclusion of partial factor scores. When MFA is used to analyze multiple tables, partial scores can be used to display how each of the tables differ from “the average view” in how they describe observations or groups. This is achieved by projecting each table separately onto the eigen-vector space. The graphical result corresponds to a “weighting” such that the distance from the (multi-table) barycenter varies per table. The graph below shows the partial factor scores for the BFI, SAM and OSIQ. Tables differ in how they represent the groups. SAM and OSIQ are mostly in agreement and better differentiate between groups by pulling the means apart. BFI, however, displays a central tendency, which contrasts with the other tables. This suggests that excluding the BFI would lead to higher group discrimination: for example, a BADA (or MUBADA) with either (or both) of these two tables would lead to higher classification accuracy than the BFI. Thus, MFA results here suggest further implementations and analyses. 8.7 Contributions The barplots below show contributions for Components 1 and 2 separately. As predicted, major contributions for Component 1 include Episodic Memory, Future Prospection and Object Imagery items, together with a relatively smaller contribution from Openness items. 8.8 Summary MFA allows for the examination of multiple data tables at the same time by revealing the overall correlation patterns between their variables. If factorial designs are part of the experiment, MFA may be used to analyze group differences by bootstrapping confidence intervals. The present analyses showed that one dimension—composed of memory, imagery and openness items—accounts for a relatively high proportion of variance in the current experimental design. Openness is moderately correlated to Episodic Memory and Future Prospection relative to Object Imagery, whose correlation with memory items is the strongest in the whole correlation matrix. "],
["distatis.html", "Chapter 9 DiSTATIS 9.1 The Data 9.2 Running DiSTATIS 9.3 ScreePlot Rv Matrix 9.4 The Rv-map 9.5 Compromise factor scores 9.6 Projecting descriptors 9.7 Summary", " Chapter 9 DiSTATIS DiSTATIS is a statistical technique belonging to the family of SVD-based methods (i.e. the PCA-related analyses covered in this cookbook). It was developed to analyze sorting experiments where participants are asked to group stimuli according to any criteria. [However, the main idea has been adapted in multiple way, one of which—CovSTATIS— has the purpose of summarizing functional connectivity data (see Herve Abdi on Darthmouth Youtube page.] DiSTATIS may reveal the (implicit) criteria underlying participants choice for grouping stimuli. It does this by leveraging participant-generated descriptions and experimental factors to interpret results. 9.0.0.1 Distance matrices The data that DiSTATIS analyzes is a collection (i.e. a cube) of distance matrices. After a participant sorts stimuli into groups, a distance matrix is computed to describe the relationships between stimuli. In this stimuli-by-stimuli matrix, a 0 at the intersection of a row and colum denotes that these stimuli have been sorted into the same group (or, in the case of the diagonal, that they are the same stimuli). A 1 denotes that stimuli do not belong to the same group; a 0, that they do. 9.0.0.2 The Compromise An SVD performed on single distance matrices is called Multidimensional Scaling. However, in DiSTATIS, the data to be described is a stimuli-by-stimuli-by-participant distance cube (i.e. distance matrices for all participants combined). The aim is to find the optimal compromise between distance matrices, or to find a linear combination such that matrices that are more “average” are weighted more. This implies the examination of between-respondent patterns, meaning a correlation between matrices. 9.0.0.3 The Rv matrix The Rv coefficient quantifies the similarity between two matrices. It is equivalent to a squared correlation between two vectorized matrices. The Rv matrix in DiSTATIS contains Rv coefficients describing the similarity between respondent’s distance matrix. The eigen-decomposition of the Rv matrix (i.e. C) will provide the weights for computing the ideal compromise. 9.0.0.4 S+ The compromise matrix (i.e. the average of all distance matrices) is denoted S+. This resulting matrix is eigen-decomposed (i.e. the second eigendecomposition!) to provide a space that describes the stimuli. Also, each assessor’s data can be projected onto this subspace. Thus, systematic variations from the compromise can be grasped by analyzing the projections according to assessor’s characteristics: certain characteristics, such as gender, might contribute to a certain pattern of deviations, in men vs. women for example. 9.1 The Data The Data used in this example corresponds to an experiment in which raters from either France or South Africa were asked to sort a set of 14 wines: 7 from France and 7 from South Africa. Rater’s country of origin provides the first (quasi-) experimental factor in the design. The second (actual) experimental factor corresponds to whether raters were shown labels for the wines or not. In the table below, rows represent wines while columns represent each judge; numbers in each cell represent the (arbitrary) groups used by each jugde to describe the wines. ## J1 J2 J3 J4 J5 J6 J7 J8 J9 ## FCAR 2 2 2 2 2 3 4 3 4 ## SRUD 1 1 6 5 2 3 4 4 3 ## FBAU 1 1 5 3 4 2 2 2 2 ## FROC 1 1 3 2 2 2 3 3 2 ## SFED 1 2 4 4 2 2 3 3 2 ## SREY 3 1 5 2 3 2 3 1 6 ## SKLE 1 1 5 5 2 3 1 2 6 ## FCLL 2 2 2 4 2 3 4 3 4 ## FCLP 2 1 3 5 1 1 4 4 1 ## SRAD 3 1 2 1 1 1 1 1 1 9.2 Running DiSTATIS DiSTATIS is executed by the ‘distatis’ function in the DistatisR package (installed from from Dr. HerveAbdi github: ‘HerveAbdi’). The “distance cube” is calculated from the sort data with the function ‘DistanceFromSort’ from the same package. # Run Distatis ------------------------------------------------------------ distanceCube = DistanceFromSort(sort.data) resDistatis = distatis(distanceCube) 9.3 ScreePlot Rv Matrix The scree plot of the Rv matrix shows that respondents vary across one dimension. 9.4 The Rv-map The Rv-map provides a graphical description of raters. By coloring according to experimental or subject variables overall patterns driving differences can be revealed at this stage. The graph shows French raters as blue and South African raters as green. Ellipses representing confidence intervals for group means show that groups differ in how they rated the wines. Component 1 of the Rv matrix always represents communality. Thus, the South African raters were more common with the total of participants while the French raters tended to be more unique. This suggests an interesting trend that will be relevant when graphing compromise factor scores. The same Rv-map color-coded by condition is graphed below to reveal the presence (or absence) of experimental effects. The overlap ellipses show that whether a subject was shown a labeled or an unlabeled wine did not significantly affect how they rated that wine. 9.5 Compromise factor scores Each assessor’s factor score on the first of the Rv-map eigenvalues is used to compute their weight in the calculation of the compromise. Thus, each assessor’s distance matrix contributes to the compromise depending on this weight. What results is the S+ matrix (i.e. compromise matrix) which is then eigen-decomposed to provide the average perceptual map of the stimuli (i.e wines). 9.5.0.1 Scree plot for the compromise The Rv scree plot shown above displays the results from the eigen-decomposition of the Rv-matrix, which describes between-respondent relationships on how they arate stimuli. The screeplot for the compromise (shown below), describes the eigen decomposition of the average between-stimuli distances. The first two dimensions explaining 24% of the variance are displayed in what follows. 9.5.0.2 Factor Map ## [1] Bootstrap On Factor Scores. Iterations #: ## [2] 1000 The factor map shows a clear effect where South African wines (starting with S) are clearly separated from French wines on Dimension 1. Below, the added confidence intervals suggest small areas of overlap between some wines of the same country (but not between wines of different countries). Dimension 2 appears to distinguish between two classes of South African wine; also, a couple of french wines are similar to South African wines that load negatively onto component two. 9.5.0.3 Partial Factor Maps The Rv-map showed a statistically significant difference between French and South African raters. To explore how country of origin (or any experimental or quasi-experimental factor) contributes to the compromise, partial factor maps may be graphed. The lines stretching out from the (stimuli) factor scores, are the partial factor scores representing the average rating for participants of each country. For example, ‘FR’ represents how the average French participant rated each wine. By interpreting the overall trend or trends specific to each item, more information can be gathered on what distinguished French from South African wine raters. In this case, there is a strong effect such that French raters tend to be closer toward the barycenter than South African raters. Thus, the French participants view each wine as more ‘average’ while South African participants see more uniqueness in the wines. 9.6 Projecting descriptors A key way to leverage sorting data is to ask participants to describe stimuli they have grouped together. Recording how many times each descriptors applies to each wine, provides data that can be later projected onto the eigenvector space of a DiSTATIS analysis. The distance between each descriptor and each wine becomes interpretable in the same way that the distance between rows and columns are interpretable in Correspondence Analysis. ## Warning: Removed 1 rows containing missing values (geom_text_repel). The graph shows the descriptions from the french raters projected onto the compromise. The terms ‘agréable’ and ‘desagréable’ sitting at opposite ends define component 1: French raters tended to like french wines better than South African wines. They considered that good wines are fruity and dry. They also considered the South African wines as either bitter or rough. Further, for french people, component two seems to capture the sweetness of the wine. Below are the english descriptors projected onto the same space. ## Warning: Removed 1 rows containing missing values (geom_text_repel). The graph shows some overlap between descriptors used by French and South African raters. First, descriptions of acidic and fruity fall on the same region. Second, sweetness loads strongly on component 2, being opposite to acidic. Interestengly, South African raters perceive South African wines as either citric (or tasting like cooked vegetables?) or tasting like dried fruit. The descriptor of ‘dried fruit’ is opposite to descriptors of ‘fruit’, ‘tropical fruit’ and ‘white fruit’, revealing that they are negatively correlated. 9.7 Summary DiSTATIS is used to analyze sorting data. The procedure in DiSTATIS involves computing distances based on the sorting of data and executing two separate eigendecompositions (one on the correlation between raters, one on the average perceptual map—i.e. compromise). In this example, DiSTATIS was used to reveal that South African wines are perceived as different than French wines (or worse-tasting, as the French raters would have it). Also, French raters tended to give more average ratings for the wines than did South African raters. In relation to South African wines, there appeared to be two classes: sweet wines vs citric wines; French wines however, were perceived as more fruity and a few of these wines were sweet. "]
]
